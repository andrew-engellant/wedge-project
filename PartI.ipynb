{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA: Wedge Process\n",
    "This repository builds on our Wedge Exploration exercise. This exercise will help you carry out the Wedge project at an A level.\n",
    "\n",
    "You'll write code that carries out the following steps:\n",
    "\n",
    "Create an empty data frame called wedge_summary with the following columns: file_name, num_rows, num_cards, num_dates\n",
    "Iterate over the zip files that hold the Wedge transaction files\n",
    "Unzip each file one at a time (so this will be part of a for loop)\n",
    "Use the CSV sniffer to determine the delimiter and whether or not there is a header row.\n",
    "Read, or attempt to read, the file into a Pandas dataframe, using the delimiter and handling headers correctly.\n",
    "For each file, store a row in wedge_summary that holds the values listed above. num_cards should be the unique card numbers in the file and num_dates should be the number of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/WedgeZipOfZips_small/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing block of code unzips each zipped file and saves the unzipped file in the 'extracted' folder under the parents directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)  # Construct full path\n",
    "    if os.path.isfile(file_path) and 'transArchive_' in file_path:  # Check if it's a file, not a directory\n",
    "        with zipfile.ZipFile(file_path) as my_zip:\n",
    "            for zipped_file in my_zip.namelist():\n",
    "                my_zip.extract(zipped_file, path='data/WedgeZipOfZips_small/extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze the CSV file using csv.Sniffer\n",
    "def sniff_csv(file_path):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        # Read a sample of the file to allow sniffing\n",
    "        sample = csvfile.read(1024 * 4)\n",
    "        # Create a Sniffer object\n",
    "        sniffer = csv.Sniffer()\n",
    "        # Determine if there is a header\n",
    "        has_header = sniffer.has_header(sample)\n",
    "        # Sniff the delimiter\n",
    "        dialect = sniffer.sniff(sample)\n",
    "        return dialect.delimiter, has_header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify NULL characters\n",
    "for file in os.listdir('data/WedgeZipOfZips_small/extracted'):\n",
    "    file_path = os.path.join('data/WedgeZipOfZips_small/extracted/', file)\n",
    "    if os.path.isfile:\n",
    "        with open(file_path, 'r') as infile:\n",
    "            # Read the entire content of the file\n",
    "            content = infile.read()\n",
    "\n",
    "        # Replace occurrences of '/N' and '//N' with 'NULL'\n",
    "        modified_content = content.replace(r'\\N', 'NULL').replace(r'\\\\N', 'NULL')\n",
    "\n",
    "        # Open the same file in write mode to overwrite it with the modified content\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            # Write the modified content back to the file\n",
    "            outfile.write(modified_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty summary table\n",
    "wedge_summary = pd.DataFrame(columns = ['file_name', 'num_rows', 'num_cards', 'num_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/WedgeZipOfZips_small/extracted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description',\n",
    "       'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity',\n",
    "       'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax',\n",
    "       'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount',\n",
    "       'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty',\n",
    "       'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType',\n",
    "       'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag',\n",
    "       'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no',\n",
    "       'store', 'branch', 'match_id', 'trans_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next chunk opens each csv in the extracted folder and adds meta data into the summary df, than exports table as summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []  # List to hold summary data\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Sniff the CSV file\n",
    "        delimiter, has_header = sniff_csv(file_path)\n",
    "        \n",
    "        if not has_header:\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, header=None, names=col_names)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, header=0)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces\n",
    "\n",
    "        # Replace null characters with uniform null value\n",
    "        df.replace({'/N': 'NULL', '//N': 'NULL'}, inplace=True)\n",
    "        \n",
    "        # Extract the columns of interest\n",
    "        num_rows = df.shape[0]\n",
    "        num_cards = df['card_no'].nunique() if 'card_no' in df.columns else 0\n",
    "        \n",
    "        df['datetime'] = pd.to_datetime(df['datetime']) # CHANGE TO DATETIME\n",
    "        num_dates = df['datetime'].dt.date.nunique() if 'datetime' in df.columns else 0\n",
    "\n",
    "        # Add the file summary to the summary list\n",
    "        summary_data.append({'file_name': file, 'num_rows': num_rows, 'num_cards': num_cards, 'num_dates': num_dates})\n",
    "\n",
    "# Create a DataFrame from the summary list\n",
    "wedge_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save the summary to CSV\n",
    "wedge_summary.to_csv('data/WedgeZipOfZips_small/summary.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your CSV file\n",
    "csv_file_path = \"data/WedgeZipOfZips_small/extracted/transArchive_201001_201003_small.csv\"\n",
    "zip_file_path = \"data/WedgeZipOfZips_small/transArchive_201001_201003_small.zip\"\n",
    "\n",
    "# Create a zip file containing the CSV file\n",
    "with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_file_path, os.path.basename(csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Pandas DataFrame\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "def cast_dataframe_to_schema(df, schema):\n",
    "    for field in schema:\n",
    "        column_name = field.name\n",
    "        field_type = field.field_type\n",
    "\n",
    "        if column_name in df.columns:\n",
    "            if field_type == \"STRING\":\n",
    "                df[column_name] = df[column_name].astype(str)\n",
    "            elif field_type == \"FLOAT\":\n",
    "                df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "            elif field_type == \"BOOLEAN\":\n",
    "                df[column_name] = df[column_name].astype(bool)\n",
    "            elif field_type == \"TIMESTAMP\":\n",
    "                df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing transArchive_201612_small.csv: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/engellantwedge2024/jobs?uploadType=multipart: Invalid field name \" .1\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names\n",
      "Error processing transArchive_201401_201403_inactive_small.csv: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/engellantwedge2024/jobs?uploadType=multipart: Invalid field name \"datetime;\"register_no\";\"emp_no\";\"trans_no\";\"upc\";\"description\";\"trans_type\";\"trans_subtype\";\"trans_status\";\"department\";\"quantity\";\"Scale\";\"cost\";\"unitPrice\";\"total\";\"regPrice\";\"altPrice\";\"tax\";\"taxexempt\";\"foodstamp\";\"wicable\";\"discount\";\"memDiscount\";\"discountable\";\"discounttype\";\"voided\";\"percentDiscount\";\"ItemQtty\";\"volDiscType\";\"volume\";\"VolSpecial\";\"mixMatch\";\"matched\";\"memType\";\"staff\";\"numflag\";\"itemstatus\";\"tenderstatus\";\"charflag\";\"varflag\";\"batchHeaderID\";\"local\";\"organic\";\"display\";\"receipt\";\"card_no\";\"store\";\"branch\";\"match_id\";\"trans_id\"\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names\n",
      "Error processing transArchive_201606_small.csv: 400 POST https://bigquery.googleapis.com/upload/bigquery/v2/projects/engellantwedge2024/jobs?uploadType=multipart: Invalid field name \" .1\". Fields must contain the allowed characters, and be at most 300 characters long. For allowed characters, please refer to https://cloud.google.com/bigquery/docs/schemas#column_names\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_OperationNotComplete\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/future/polling.py:120\u001b[0m, in \u001b[0;36mPollingFuture._done_or_raise\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone(retry\u001b[38;5;241m=\u001b[39mretry):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _OperationNotComplete()\n",
      "\u001b[0;31m_OperationNotComplete\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     df \u001b[38;5;241m=\u001b[39m cast_dataframe_to_schema(df, schema) \u001b[38;5;66;03m# Ensure dataframe follows schema\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mload_table_from_dataframe(df, table_ref)\n\u001b[0;32m---> 19\u001b[0m     job\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob\u001b[38;5;241m.\u001b[39moutput_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py:966\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    965\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m--> 966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(_AsyncJob, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/future/polling.py:256\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_VALUE, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, polling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the result of the operation.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    This method will poll for operation status periodically, blocking if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m            the timeout is reached before the operation completes.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/future/polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture._blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    134\u001b[0m     polling \u001b[38;5;241m=\u001b[39m polling\u001b[38;5;241m.\u001b[39mwith_timeout(timeout)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     polling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done_or_raise)(retry\u001b[38;5;241m=\u001b[39mretry)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation did not complete within the designated timeout of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:164\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         _retry_error_helper(\n\u001b[1;32m    154\u001b[0m             exc,\n\u001b[1;32m    155\u001b[0m             deadline,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m             timeout,\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(sleep)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dir = 'data/WedgeZipOfZips_small/extracted/'\n",
    "\n",
    "project_id = 'engellantwedge2024'  # Change this to your project ID\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            dataset_id = 'wedge_transactions_small'\n",
    "            table_id = os.path.splitext(file)[0] #Create table name\n",
    "            table_ref = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "            \n",
    "            df = pd.read_csv(file_path) #Read the CSV file into a DataFrame\n",
    "            \n",
    "            df = cast_dataframe_to_schema(df, schema) # Ensure dataframe follows schema\n",
    "            \n",
    "            job = client.load_table_from_dataframe(df, table_ref)\n",
    "            job.result()\n",
    "            print(f'Loaded {job.output_rows} rows into {table_ref}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 rows into engellantwedge2024.wedge_transactions_small.test_table.\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a Pandas DataFrame\n",
    "csv_file_path = 'data/WedgeZipOfZips_small/extracted/transArchive_201001_201003_small.csv'  # Change this to your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize a BigQuery client\n",
    "project_id = 'engellantwedge2024'  # Change this to your project ID\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Define the dataset and table where you want to upload the DataFrame\n",
    "dataset_id = 'wedge_transactions_small'  # Change this to your BigQuery dataset ID\n",
    "table_id = 'test_table'  # Change this to your BigQuery table ID\n",
    "\n",
    "# Define the fully qualified table ID\n",
    "table_ref = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Upload the DataFrame to BigQuery\n",
    "# If the table doesn't exist, it will be created\n",
    "job = client.load_table_from_dataframe(df, table_ref)\n",
    "\n",
    "# Wait for the job to complete\n",
    "job.result()\n",
    "\n",
    "print(f\"Loaded {job.output_rows} rows into {table_ref}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'engellantwedge2024'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.project\n",
    "pd.da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'engellantwedge2024'  # Change this to your project ID\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Set the path to your file\n",
    "zip_file_path = \"data/WedgeZipOfZips_small/transArchive_201001_201003_small.zip\"\n",
    "\n",
    "# Specify project, dataset, and table details\n",
    "project_id = \"engellantwedge2024\"\n",
    "dataset_id = \"wedge_transactions_small\"\n",
    "table_id = 'test_table'\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Specify job configuration with the defined schema\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Skip the header row\n",
    "    schema=schema,  # Use the specified schema\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load the CSV file into BigQuery\n",
    "    with open(zip_file_path, \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "    job.result()  # Wait for the job to complete\n",
    "\n",
    "    # Print the number of rows loaded\n",
    "    print(f\"Loaded {job.output_rows} rows into {table_ref}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
