{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA: Wedge Process\n",
    "This repository builds on our Wedge Exploration exercise. This exercise will help you carry out the Wedge project at an A level.\n",
    "\n",
    "You'll write code that carries out the following steps:\n",
    "\n",
    "Create an empty data frame called wedge_summary with the following columns: file_name, num_rows, num_cards, num_dates\n",
    "Iterate over the zip files that hold the Wedge transaction files\n",
    "Unzip each file one at a time (so this will be part of a for loop)\n",
    "Use the CSV sniffer to determine the delimiter and whether or not there is a header row.\n",
    "Read, or attempt to read, the file into a Pandas dataframe, using the delimiter and handling headers correctly.\n",
    "For each file, store a row in wedge_summary that holds the values listed above. num_cards should be the unique card numbers in the file and num_dates should be the number of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import csv\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/WedgeZipOfZips_small/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing block of code unzips each zipped file and saves the unzipped file in the 'extracted' folder under the parents directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)  # Construct full path\n",
    "    if os.path.isfile(file_path) and 'transArchive_' in file_path:  # Check if it's a file, not a directory\n",
    "        with zipfile.ZipFile(file_path) as my_zip:\n",
    "            for zipped_file in my_zip.namelist():\n",
    "                my_zip.extract(zipped_file, path='data/WedgeZipOfZips_small/extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze the CSV file using csv.Sniffer\n",
    "def sniff_csv(file_path):\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        # Read a sample of the file to allow sniffing\n",
    "        sample = csvfile.read(1024 * 4)\n",
    "        # Create a Sniffer object\n",
    "        sniffer = csv.Sniffer()\n",
    "        # Determine if there is a header\n",
    "        has_header = sniffer.has_header(sample)\n",
    "        # Sniff the delimiter\n",
    "        dialect = sniffer.sniff(sample)\n",
    "        return dialect.delimiter, has_header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify NULL characters\n",
    "for file in os.listdir('data/WedgeZipOfZips_small/extracted'):\n",
    "    file_path = os.path.join('data/WedgeZipOfZips_small/extracted/', file)\n",
    "    if os.path.isfile:\n",
    "        with open(file_path, 'r') as infile:\n",
    "            # Read the entire content of the file\n",
    "            content = infile.read()\n",
    "\n",
    "        # Replace occurrences of '/N' and '//N' with 'NULL'\n",
    "        modified_content = content.replace(r'\\N', 'NULL').replace(r'\\\\N', 'NULL')\n",
    "\n",
    "        # Open the same file in write mode to overwrite it with the modified content\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            # Write the modified content back to the file\n",
    "            outfile.write(modified_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty summary table\n",
    "wedge_summary = pd.DataFrame(columns = ['file_name', 'num_rows', 'num_cards', 'num_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/WedgeZipOfZips_small/extracted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description',\n",
    "       'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity',\n",
    "       'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax',\n",
    "       'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount',\n",
    "       'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty',\n",
    "       'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType',\n",
    "       'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag',\n",
    "       'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no',\n",
    "       'store', 'branch', 'match_id', 'trans_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next chunk opens each csv in the extracted folder and adds meta data into the summary df, than exports table as summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []  # List to hold summary data\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        # Sniff the CSV file\n",
    "        delimiter, has_header = sniff_csv(file_path)\n",
    "        \n",
    "        if not has_header:\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, header=None, names=col_names)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, delimiter=delimiter, header=0)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces\n",
    "\n",
    "        # Replace null characters with uniform null value\n",
    "        df.replace({'/N': 'NULL', '//N': 'NULL'}, inplace=True)\n",
    "        \n",
    "        # Extract the columns of interest\n",
    "        num_rows = df.shape[0]\n",
    "        num_cards = df['card_no'].nunique() if 'card_no' in df.columns else 0\n",
    "        \n",
    "        df['datetime'] = pd.to_datetime(df['datetime']) # CHANGE TO DATETIME\n",
    "        num_dates = df['datetime'].dt.date.nunique() if 'datetime' in df.columns else 0\n",
    "\n",
    "        # Add the file summary to the summary list\n",
    "        summary_data.append({'file_name': file, 'num_rows': num_rows, 'num_cards': num_cards, 'num_dates': num_dates})\n",
    "\n",
    "# Create a DataFrame from the summary list\n",
    "wedge_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save the summary to CSV\n",
    "wedge_summary.to_csv('data/WedgeZipOfZips_small/summary.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
